1.没有判断编码,处理的不够详尽
2.对于如何处理 unk_token的问题。现在是在加载预向量的同时，将其随机初始化为一个嵌入
  对于所有的unk_token,随机初始化为不同的值。使用  0-1 之间的均匀分布
3.迭代器生成：现在只考虑字符级别的。 生成的tensor 为 torch.int32
4.每一个都读了那么多的数据，有点担心内存会不会因此爆掉
   解决办法：
     你可以使用迭代器逐行读取文件中的文本，而不是一次性读取所有内容。这会再次为你节省大量内存，尤其是在文件很大的情况下。
5.因为不止一个输入的表示，每个工程确认一个输入的表示，然后实现不同种的transformer，this is ok。
6.在原文中，对Embedding weights 做了归一化处理，multiply by sqrt(dk) 不是很懂为什么？
7.产生了一个新的疑问，在函数传过去一些参数之后，在本函数内部修改参数会不会影响到原来的参数值？
8.对于 layer_norm 的疑问，预测的时候应该怎么做
9.对于layer_norm层和 crf_decoder中的nn.parameter 在赋值的时候，我们是不是不能用torch.ones,先写上，后续再说。
10. 计算二进制mask，用的是 float类型
11. 为了保证可见性，设置torch.manual_seed 是什么
12.文中的log底数到底是多少？暂且认为是e 自然对数
如果log是e,为什么要多此一举呢？
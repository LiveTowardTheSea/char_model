2.对于如何处理 unk_token的问题。现在是在加载预向量的同时，将其随机初始化为一个嵌入
  对于所有的unk_token,随机初始化为不同的值。使用  0-1 之间的均匀分布
3.迭代器生成：现在只考虑字符级别的。 生成的tensor 为 torch.int32
5.因为不止一个输入的表示，每个工程确认一个输入的表示，然后实现不同种的transformer，this is ok。
6.在原文中，对Embedding weights 做了归一化处理，multiply by sqrt(dk) 不是很懂为什么？
7.产生了一个新的疑问，在函数传过去一些参数之后，在本函数内部修改参数会不会影响到原来的参数值？
10. 计算二进制mask，用的是 float类型
11. 为了保证可见性，设置torch.manual_seed 是什么
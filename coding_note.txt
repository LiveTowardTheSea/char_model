2.对于如何处理 unk_token的问题。现在是在加载预向量的同时，将其随机初始化为一个嵌入
  对于所有的unk_token,随机初始化为不同的值。使用  0-1 之间的均匀分布
3.迭代器生成：现在只考虑字符级别的。 生成的tensor 为 torch.int32
5.因为不止一个输入的表示，每个工程确认一个输入的表示，然后实现不同种的transformer，this is ok。
6.在原文中，对Embedding weights 做了归一化处理，multiply by sqrt(dk) 不是很懂为什么？
7.产生了一个新的疑问，在函数传过去一些参数之后，在本函数内部修改参数会不会影响到原来的参数值？
10. 计算二进制mask，用的是 float类型
11. 为了保证可见性，设置torch.manual_seed 是什么


4.24
1.关于loss 以及在验证集上结果的改善我们要绘图
3.代码逻辑是否正确，求导是否正确，这个我们是否需要考证
4.对于模型在测试数据及上，我们应该让他生成一个结果。这样看看我们的代码把输入搞成了什么
5.对于预测的时候，因为我们是用了人尽皆知的self-attention,我们应该输出每个头的那个图
6.下次训练模型需要注意的问题： dev set 不够长  lr 衰减的太快了，没显示在文件里，
7.train 结果甚至不如dev,这又是什么毛病！！是不是penalty drop 太大了 还是lr太小了，衰减的！